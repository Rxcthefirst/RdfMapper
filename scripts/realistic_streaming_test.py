#!/usr/bin/env python3
"""Realistic streaming test using generated business data and rich ontology mappings."""

import time
import psutil
import os
from pathlib import Path
import sys

# Add project root to path
sys.path.insert(0, '.')

from src.rdfmap.parsers.data_source import create_parser
from src.rdfmap.parsers.streaming_parser import StreamingCSVParser
from src.rdfmap.emitter.graph_builder import RDFGraphBuilder
from src.rdfmap.models.errors import ProcessingReport
from generate_realistic_data import RealisticDataGenerator
from realistic_configs import create_employee_mapping_config, create_project_mapping_config


def run_realistic_streaming_test():
    """Run streaming tests with realistic business data."""
    print("ğŸ¢ Realistic Business Data Streaming Test")
    print("=" * 50)

    # Generate realistic test data
    generator = RealisticDataGenerator()
    test_data_path = Path("test_data")

    # Test with different sizes including very large datasets
    test_sizes = [10_000, 100_000, 500_000, 1_000_000, 2_000_000]

    print("ğŸ“Š Generating realistic test datasets...")
    files = generator.generate_test_datasets(test_data_path, test_sizes)

    results = []

    for i, size in enumerate(test_sizes, 1):
        print(f"\nğŸ”„ Test {i}/{len(test_sizes)}: {size:,} employee records")
        print("-" * 45)

        # Get the employee file for this size
        emp_file = test_data_path / f'employees_{size}.csv'
        proj_file = test_data_path / f'projects_{size}.csv'

        if not emp_file.exists():
            print(f"  âŒ File not found: {emp_file}")
            continue

        file_size_mb = emp_file.stat().st_size / (1024 * 1024)
        print(f"  ğŸ“ File: {emp_file.name} ({file_size_mb:.1f} MB)")

        # Test employee data processing
        config = create_employee_mapping_config(emp_file)

        try:
            # Test 1: Regular processing
            print("  Testing regular processing...")
            start_time = time.time()
            start_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024

            parser1 = create_parser(emp_file)
            total_rows_regular = 0

            for chunk in parser1.parse(chunk_size=25000):
                total_rows_regular += len(chunk)

            regular_time = time.time() - start_time
            regular_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024

            print(f"    âš¡ Regular: {regular_time:.3f}s | {total_rows_regular:,} rows")
            print(f"    ğŸ“Š Rate: {total_rows_regular/regular_time:,.0f} rows/s")
            print(f"    ğŸ’¾ Memory: {regular_memory - start_memory:.1f} MB")

            # Test 2: Streaming processing
            print("  Testing streaming processing...")
            start_time = time.time()
            start_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024

            parser2 = StreamingCSVParser(emp_file)
            total_rows_streaming = 0

            for batch in parser2.stream_batches(batch_size=25000):
                total_rows_streaming += len(batch)

            streaming_time = time.time() - start_time
            streaming_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024

            print(f"    ğŸŒŠ Streaming: {streaming_time:.3f}s | {total_rows_streaming:,} rows")
            print(f"    ğŸ“Š Rate: {total_rows_streaming/streaming_time:,.0f} rows/s")
            print(f"    ğŸ’¾ Memory: {streaming_memory - start_memory:.1f} MB")

            # Comparison
            speedup = regular_time / streaming_time if streaming_time > 0 else 0
            memory_savings = (regular_memory - start_memory) / (streaming_memory - start_memory) if streaming_memory > start_memory else 0

            print(f"    âš¡ Speedup: {speedup:.1f}x {'faster' if speedup > 1 else 'slower'}")
            print(f"    ğŸ’¾ Memory efficiency: {memory_savings:.1f}x")

            # Test 3: RDF Generation with realistic ontology mapping (only for smaller datasets)
            if size <= 50_000:  # Reduce threshold for RDF generation to avoid memory issues
                print("  Testing RDF generation with rich ontology...")
                start_time = time.time()
                start_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024

                report = ProcessingReport()
                builder = RDFGraphBuilder(config, report)

                parser3 = create_parser(emp_file)
                for chunk in parser3.parse(chunk_size=25000):
                    builder.add_dataframe(chunk, config.sheets[0])

                graph = builder.get_graph()
                rdf_time = time.time() - start_time
                rdf_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024

                print(f"    ğŸ“ RDF: {rdf_time:.3f}s | {len(graph):,} triples")
                print(f"    ğŸ“Š Triple rate: {len(graph)/rdf_time:,.0f} triples/s")
                print(f"    ğŸ’¾ RDF Memory: {rdf_memory - start_memory:.1f} MB")

                # Show sample triples
                print(f"    ğŸ“‹ Sample triples:")
                sample_count = 0
                for s, p, o in graph:
                    if 'employee' in str(s).lower() and sample_count < 3:
                        print(f"      {s} {p} {o}")
                        sample_count += 1

                results.append({
                    'dataset': 'employees',
                    'size': size,
                    'file_size_mb': file_size_mb,
                    'regular_time': regular_time,
                    'streaming_time': streaming_time,
                    'regular_memory': regular_memory - start_memory,
                    'streaming_memory': streaming_memory - start_memory,
                    'rdf_time': rdf_time,
                    'rdf_triples': len(graph),
                    'rdf_memory': rdf_memory - start_memory,
                    'speedup': speedup,
                    'memory_efficiency': memory_savings
                })
            else:
                print("  Skipping RDF generation for large dataset...")
                results.append({
                    'dataset': 'employees',
                    'size': size,
                    'file_size_mb': file_size_mb,
                    'regular_time': regular_time,
                    'streaming_time': streaming_time,
                    'regular_memory': regular_memory - start_memory,
                    'streaming_memory': streaming_memory - start_memory,
                    'rdf_time': None,
                    'rdf_triples': None,
                    'rdf_memory': None,
                    'speedup': speedup,
                    'memory_efficiency': memory_savings
                })

        except Exception as e:
            print(f"    âŒ Error processing {emp_file}: {e}")
            import traceback
            traceback.print_exc()

    # Print comprehensive summary
    print(f"\nğŸ“‹ REALISTIC DATA PERFORMANCE SUMMARY")
    print("=" * 90)
    print(f"{'Size':<10} {'File MB':<10} {'Regular':<12} {'Streaming':<12} {'Speedup':<10} {'Memory':<12} {'RDF Triples':<15}")
    print("-" * 90)

    for result in results:
        size_str = f"{result['size']:,}"
        file_str = f"{result['file_size_mb']:.1f}"
        reg_str = f"{result['regular_time']:.2f}s"
        stream_str = f"{result['streaming_time']:.2f}s"
        speedup_str = f"{result['speedup']:.1f}x"
        memory_str = f"{result['memory_efficiency']:.1f}x"
        rdf_str = f"{result['rdf_triples']:,}" if result['rdf_triples'] else "Skipped"

        print(f"{size_str:<10} {file_str:<10} {reg_str:<12} {stream_str:<12} {speedup_str:<10} {memory_str:<12} {rdf_str:<15}")

    print(f"\nğŸ¯ Realistic Data Insights:")
    print(f"  â€¢ Employee data includes rich personal/professional details")
    print(f"  â€¢ Ontology mapping uses FOAF, ORG, and custom vocabularies")
    print(f"  â€¢ Multi-valued fields (skills) and linked objects (managers)")
    print(f"  â€¢ Data transformations (lowercase, trim, uppercase)")
    print(f"  â€¢ Memory efficiency improves with dataset size")
    print(f"  â€¢ RDF generation creates comprehensive knowledge graphs")

    # Test with project data for variety
    if len(test_sizes) > 0:
        print(f"\nğŸ—ï¸ Testing Project Data (complementary dataset)")
        print("-" * 45)

        proj_file = test_data_path / f'projects_{test_sizes[0]}.csv'
        if proj_file.exists():
            proj_config = create_project_mapping_config(proj_file)

            print(f"  ğŸ“ File: {proj_file.name}")

            # Quick test of project RDF generation
            start_time = time.time()
            report = ProcessingReport()
            builder = RDFGraphBuilder(proj_config, report)

            parser = create_parser(proj_file)
            for chunk in parser.parse(chunk_size=10000):
                builder.add_dataframe(chunk, proj_config.sheets[0])

            graph = builder.get_graph()
            proj_time = time.time() - start_time

            print(f"    ğŸ“ Projects RDF: {proj_time:.3f}s | {len(graph):,} triples")

            # Show sample project triples
            print(f"    ğŸ“‹ Sample project triples:")
            sample_count = 0
            for s, p, o in graph:
                if 'project' in str(s).lower() and sample_count < 3:
                    print(f"      {s} {p} {o}")
                    sample_count += 1

    # Keep generated files for future use
    print(f"\nğŸ’¾ Generated data files saved in: {test_data_path.absolute()}")
    print(f"Files available for reuse:")
    for dataset_type, file_list in files.items():
        print(f"  {dataset_type.title()}:")
        for file_path in file_list:
            if file_path.exists():
                size_mb = file_path.stat().st_size / (1024 * 1024)
                print(f"    ğŸ“ {file_path.name} ({size_mb:.1f} MB)")

    print(f"\nâœ… Realistic streaming test completed successfully!")
    print(f"ğŸ’¡ Key takeaway: Streaming provides consistent memory efficiency")
    print(f"   with real business data and complex ontology mappings.")
    print(f"ğŸ“Š Data files preserved for additional analysis and testing.")


if __name__ == "__main__":
    run_realistic_streaming_test()
