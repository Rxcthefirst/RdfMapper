#!/usr/bin/env python3
"""Quick verification test to show actual row counts and basic streaming comparison."""

import time
from pathlib import Path
import sys
sys.path.insert(0, '.')

from src.rdfmap.parsers.data_source import create_parser
from src.rdfmap.parsers.streaming_parser import StreamingCSVParser


def quick_verification_test():
    """Quick test to verify data and show streaming benefits."""
    print("ğŸ“Š Dataset Verification & Quick Streaming Test")
    print("=" * 55)

    test_data_path = Path("test_data")
    test_sizes = [100_000, 500_000, 1_000_000, 2_000_000]

    print("ğŸ” Verifying actual row counts:")
    for size in test_sizes:
        emp_file = test_data_path / f'employees_{size}.csv'
        if emp_file.exists():
            # Quick row count verification
            with open(emp_file, 'r') as f:
                actual_rows = sum(1 for line in f) - 1  # Subtract header

            file_size_mb = emp_file.stat().st_size / (1024 * 1024)
            print(f"  ğŸ“„ {emp_file.name:<25} {actual_rows:,} rows, {file_size_mb:.1f} MB")

    print(f"\nğŸš€ Quick Performance Test (1M records):")
    print("-" * 45)

    # Test with 1M record file
    emp_file = test_data_path / f'employees_1000000.csv'

    if emp_file.exists():
        # Regular parsing
        print("  Testing regular parsing...")
        start_time = time.time()
        parser1 = create_parser(emp_file)
        total_rows_regular = 0
        for chunk in parser1.parse(chunk_size=100000):
            total_rows_regular += len(chunk)
        regular_time = time.time() - start_time

        # Streaming parsing
        print("  Testing streaming parsing...")
        start_time = time.time()
        parser2 = StreamingCSVParser(emp_file)
        total_rows_streaming = 0
        for batch in parser2.stream_batches(batch_size=100000):
            total_rows_streaming += len(batch)
        streaming_time = time.time() - start_time

        print(f"\nğŸ“ˆ Results for 1,000,000 employee records:")
        print(f"  âš¡ Regular:   {regular_time:.3f}s  ({total_rows_regular/regular_time:,.0f} rows/s)")
        print(f"  ğŸŒŠ Streaming: {streaming_time:.3f}s  ({total_rows_streaming/streaming_time:,.0f} rows/s)")
        print(f"  ğŸ¯ Speedup:   {regular_time/streaming_time:.2f}x")

        print(f"\nâœ… Both modes processed exactly {total_rows_regular:,} rows")
        print(f"ğŸ’¡ Data verification complete - files contain correct row counts")

    print(f"\nğŸ“ Available datasets in {test_data_path.absolute()}:")
    for file_path in sorted(test_data_path.glob("employees_*.csv")):
        size_mb = file_path.stat().st_size / (1024 * 1024)
        print(f"  ğŸ“„ {file_path.name} ({size_mb:.1f} MB)")


if __name__ == "__main__":
    quick_verification_test()
